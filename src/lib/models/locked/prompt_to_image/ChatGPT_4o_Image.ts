/**
 * üîí LOCKED MODEL: ChatGPT 4o-Image
 *
 * Group: prompt_to_image
 * Provider: kie_ai
 * Content Type: image
 *
 * ‚ö†Ô∏è DO NOT EDIT THIS FILE MANUALLY
 * This file is the single source of truth for this model
 */

import { getGenerationType } from "@/lib/models/registry";
import type { ExecuteGenerationParams } from "@/lib/generation/executeGeneration";
import { supabase } from "@/integrations/supabase/client";
import { reserveCredits } from "@/lib/models/creditDeduction";
import { GENERATION_STATUS } from "@/constants/generation-status";
import { sanitizeForStorage } from "@/lib/database/sanitization";
import { extractEdgeFunctionError } from "@/lib/utils/edge-function-error";

// MODEL CONFIGURATION
export const MODEL_CONFIG = {
  modelId: "4o-image-api",
  recordId: "3b83cee8-6164-4d98-aebe-f4eadcb3da1d",
  modelName: "ChatGPT 4o-Image",
  provider: "kie_ai",
  contentType: "prompt_to_image",
  use_api_key: "KIE_AI_API_KEY_PROMPT_TO_IMAGE",
  baseCreditCost: 3,
  estimatedTimeSeconds: 120,
  costMultipliers: { nVariants: { "1": 1, "2": 1.16666, "4": 1.3333 } },
  apiEndpoint: "/api/v1/gpt4o-image/generate",
  payloadStructure: "flat",
  maxImages: 5,
  defaultOutputs: 1,
  // UI metadata
  isActive: true,
  logoUrl: "/logos/openai.png",
  modelFamily: "OpenAI",
  variantName: "ChatGPT 4o-Image",
  displayOrderInFamily: 2,
  // Lock system
  isLocked: true,
  lockedFilePath: "src/lib/models/locked/prompt_to_image/ChatGPT_4o_Image.ts",
} as const;

// FROZEN SCHEMA
export const SCHEMA = {
  type: "object",
  // imageInputField: "filesUrl",
  properties: {
    prompt: { type: "string", renderer: "prompt", title: "Prompt" },
    size: {
      type: "string",
      title: "Aspect Ratio",
      default: "1:1",
      enum: ["1:1", "3:2", "2:3"],
    },
    nVariants: {
      type: "integer",
      title: "Number of Outputs",
      default: 1,
      enum: [1, 2, 4],
    },
    filesUrl: {
      type: "array",
      title: "Reference Images (Optional)",
      description: "Up to 5 images to use as reference. Formats: .jpeg, .jpg, .png, .webp",
      renderer: "image",
      showToUser: false,
      items: { type: "string", format: "uri" },
      maxItems: 5,
    },
    maskUrl: {
      type: "string",
      title: "Mask URL",
      description: "Mask image URL for selective editing (black=modify, white=preserve)",
      format: "uri",
      showToUser: false,
    },
    isEnhance: {
      type: "boolean",
      title: "Enhance Prompt",
      description: "Enable prompt enhancement for 3D renders",
      default: false,
      showToUser: false,
    },
    enableFallback: {
      type: "boolean",
      title: "Enable Fallback",
      description: "Fallback to backup model if GPT-4o unavailable",
      default: false,
      showToUser: false,
    },
    fallbackModel: {
      type: "string",
      title: "Fallback Model",
      default: "FLUX_MAX",
      enum: ["GPT_IMAGE_1", "FLUX_MAX"],
      showToUser: false,
    },
    uploadCn: {
      type: "boolean",
      title: "China Server",
      description: "Route uploads via China servers",
      default: false,
      showToUser: false,
    },
  },
  required: ["size"],
};

// VALIDATION
export function validate(inputs: Record<string, any>): { valid: boolean; error?: string } {
  if (!inputs.size) return { valid: false, error: "Size is required" };
  if (!inputs.prompt && (!inputs.filesUrl || inputs.filesUrl.length === 0)) {
    return { valid: false, error: "Either prompt or reference images are required" };
  }
  if (inputs.filesUrl && inputs.filesUrl.length > 5) {
    return { valid: false, error: "Maximum 5 reference images allowed" };
  }
  return { valid: true };
}

// PAYLOAD PREPARATION
export function preparePayload(inputs: Record<string, any>): Record<string, any> {
  const payload: Record<string, any> = {
    size: inputs.size || "1:1",
    nVariants: inputs.nVariants || 1,
  };

  // Add prompt if provided
  if (inputs.prompt) payload.prompt = inputs.prompt;

  // Add reference images if provided
  if (inputs.filesUrl && inputs.filesUrl.length > 0) {
    payload.filesUrl = inputs.filesUrl;
  }

  // Add optional parameters
  if (inputs.maskUrl) payload.maskUrl = inputs.maskUrl;
  if (inputs.isEnhance) payload.isEnhance = inputs.isEnhance;
  if (inputs.enableFallback) payload.enableFallback = inputs.enableFallback;
  if (inputs.fallbackModel && inputs.enableFallback) payload.fallbackModel = inputs.fallbackModel;
  if (inputs.uploadCn) payload.uploadCn = inputs.uploadCn;

  return payload;
}

// COST CALCULATION
export function calculateCost(inputs: Record<string, any>): number {
  let cost = MODEL_CONFIG.baseCreditCost;
  const nVariants = inputs.nVariants || 1;
  const multiplier =
    MODEL_CONFIG.costMultipliers.nVariants[String(nVariants) as keyof typeof MODEL_CONFIG.costMultipliers.nVariants] ||
    1;
  cost *= multiplier;
  return Math.round(cost * 100) / 100;
}

// EXECUTION
export async function execute(params: ExecuteGenerationParams): Promise<string> {
  const { prompt, modelParameters, uploadedImages, userId, uploadImagesToStorage, startPolling } = params;
  const inputs: Record<string, any> = { prompt, ...modelParameters };

  // Upload images and get URLs if provided
  if (uploadedImages.length > 0) {
    inputs.filesUrl = await uploadImagesToStorage(userId);
  }

  // Validate
  const validation = validate(inputs);
  if (!validation.valid) throw new Error(validation.error);

  // Calculate cost
  const cost = calculateCost(inputs);

  // Deduct credits
  await reserveCredits(userId, cost);

  // Create generation record with pending status (edge function will process)
  const { data: generation, error: genError } = await supabase
    .from("generations")
    .insert({
      user_id: userId,
      model_id: MODEL_CONFIG.modelId,
      model_record_id: MODEL_CONFIG.recordId,
      prompt,
      type: getGenerationType(MODEL_CONFIG.contentType),
      status: GENERATION_STATUS.PENDING,
      tokens_used: cost,
      settings: sanitizeForStorage(modelParameters),
    })
    .select()
    .single();

  if (genError || !generation) throw new Error(`Failed to create generation: ${genError?.message}`);

  // Call edge function to handle API call server-side
  const { error: funcError } = await supabase.functions.invoke("generate-content", {
    body: {
      generationId: generation.id,
      model_config: MODEL_CONFIG,
      model_schema: SCHEMA,
      prompt,
      custom_parameters: preparePayload(inputs),
    },
  });

  if (funcError) {
    await supabase.from("generations").update({ status: GENERATION_STATUS.FAILED }).eq("id", generation.id);
    const errorMessage = await extractEdgeFunctionError(funcError);
    throw new Error(errorMessage);
  }

  // Start polling
  startPolling(generation.id);

  return generation.id;
}
